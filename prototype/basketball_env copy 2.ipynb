{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import pandas as pd\n",
    "import random\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import time\n",
    "\n",
    "class BasketballEnv(gym.Env):\n",
    "\t#metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "\tdef __init__(self, render_mode=None ):\n",
    "\t\t#self.size = size  # The size of the square grid\n",
    "\t\t\n",
    "\t\tself.number_to_character = {'0' : 'Alice', '1':'Bob' , '2':'Charlie','3':'Sherlock','4':'?'}\n",
    "\t\tself.character_to_number = {v: k for k, v in self.number_to_character.items()}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\tself.number_to_place = {'0' : 'HomeB', '1':'BasketballCourt' , '2':'Downtown','3':'?'}\n",
    "\t\tself.place_to_number = {v: k for k, v in self.number_to_place.items()}\n",
    "\n",
    "\t\tself.number_to_item = {'0' : 'Basketball', '1':'Bat','2':'?' }\n",
    "\t\tself.item_to_number = {v: k for k, v in self.number_to_item.items()}\n",
    "\n",
    "\t\tself.number_to_crime = {'0' : 'Theft', '1':'Murder','2':'?' }\n",
    "\t\tself.crime_to_number = {v: k for k, v in self.number_to_crime.items()}\n",
    "\n",
    "\t\t\n",
    "\t\t#self._nb_features = 39\n",
    "\t\tself.score = 0\n",
    "\t\tself.characters = ['Alice', 'Bob', 'Charlie','Sherlock']\n",
    "\t\tself.items = ['Basketball','Bat']\n",
    "\t\tself.places= ['HomeB','BasketballCourt','Downtown']\n",
    "\t\tself.crimes = ['Theft','Murder']\n",
    "\n",
    "\t\tself.utilities = {\n",
    "\t\t\t'Alice':'1 - angry(Alice);\\n',\n",
    "\t\t\t'Bob':'3 - (sum(c : citizen) angry(c));\\n',\n",
    "\t\t\t'Charlie':'if(!alive(Alice)) 1 else 0;\\n',\n",
    "\t\t\t'Sherlock':'(sum(c : citizen) underArrest(c)); \\n',\n",
    "\t\t}\n",
    "\n",
    "\t\tself.utilities1 = {\n",
    "\t\t\t'Alice':'1 - angry(Alice);\\n',\n",
    "\t\t\t'Bob':'3 - (sum(c : citizen) angry(c));\\n',\n",
    "\t\t\t'Charlie':'if(!alive(Alice)) 1 else 0;\\n',\n",
    "\t\t\t'Sherlock':'(sum(c : citizen) underArrest(c)); \\n',\n",
    "\t\t}\n",
    "\n",
    "\t\tself.utilities2 = {\n",
    "\t\t\t'Alice':'1 - angry(Alice);\\n',\n",
    "\t\t\t'Bob':'3 - (sum(c : citizen) angry(c));\\n',\n",
    "\t\t\t'Charlie':'if(!alive(Alice)) 1 else 0;\\n',\n",
    "\t\t\t'Sherlock':' (sum(p : place) searched(p)); \\n',\n",
    "\n",
    "\n",
    "\t\t}\n",
    "\t\t\"\"\"\n",
    "\t\tself.characters = ['0','1','2','3']\n",
    "\t\tself.items = ['0','1']\n",
    "\t\tself.places= ['0','1','2']\n",
    "\t\tself.crimes = ['0','1']\n",
    "\t\tself.utilities = {\n",
    "\t\t\t'0':'1 - angry(Alice);\\n',\n",
    "\t\t\t'1':'3 - (sum(c : citizen) angry(c));\\n',\n",
    "\t\t\t'2':'if(!alive(Alice)) 1 else 0;\\n',\n",
    "\t\t\t'3':'(sum(c : citizen) underArrest(c)) + (sum(p : place) searched(p)); \\n',\n",
    "\t\t}\n",
    "\t\t\"\"\"\n",
    "\t\tself.acting_character = self.characters[0]\n",
    "\t\tself.additional_utility = None\n",
    "\t\tself.df_effects = pd.read_csv('basketball_effects_nn.csv')\n",
    "\t\tself.file = 'rl_planner.txt'\n",
    "\t\t\n",
    "\t\tself.randomize_df()\n",
    "\t\tself.df = self.change_number_to_cat(self.df)\n",
    "\t\tself.create_file(self.df)\n",
    "\t\tself.no_solution = 0\n",
    "\t\tself._nb_features = len(self.df.index)\n",
    "\t\t# Observations are dictionaries with the agent's and the target's location.\n",
    "\t\t# Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "\t\tself.observation_space = spaces.Box(\n",
    "\t\t\t0,\n",
    "\t\t\t5,\n",
    "\t\t\tshape = [self._nb_features]\n",
    "\t\t)\n",
    "\n",
    "\t\tself.target_feature = None\n",
    "\t\tself.target_value = None\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t# We have 4 actions, corresponding to choosing character\n",
    "\t\tself.action_space = spaces.Discrete(6)\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tThe following dictionary maps abstract actions from `self.action_space` to\n",
    "\t\tthe direction we will walk in if that action is taken.\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tself._action_to_direction = {\n",
    "\t\t\t0: 'char0',\n",
    "\t\t\t1: 'char1',\n",
    "\t\t\t2: 'char2',\n",
    "\t\t\t3: 'char3',\n",
    "\t\t\t4: 'Sherlock_Utility_1',\n",
    "\t\t\t5: 'Sherlock_Utility_2'\n",
    "\t\t}\n",
    "\n",
    "\t\tprint('nb',len(self.df.index))\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "\n",
    "\tdef change_number_to_cat(self,row):\n",
    "\t\tfor index,value in row.items():\n",
    "\t\t\tif ('acting_character' in index)  or ('has' in index) :\n",
    "\t\t\t\t\n",
    "\t\t\t\trow[index] = self.number_to_character[str(value)]\n",
    "\t\t\t\n",
    "\t\t\telif 'at' in index:\n",
    "\t\t\t\trow[index] = self.number_to_place[str(value)]\n",
    "\n",
    "\t\treturn row\n",
    "\n",
    "\tdef change_cat_to_number(self,row):\n",
    "\t\tfor index,value in row.items():\n",
    "\t\t\tif ('acting_character' in index) or ('has' in index):\n",
    "\t\t\t\tif str(value) in self.character_to_number.keys():\n",
    "\t\t\t\t\trow[index] = self.character_to_number[str(value)]\n",
    "\t\t\t\n",
    "\t\t\telif 'at' in index:\n",
    "\t\t\t\tif str(value) in self.place_to_number.keys():\n",
    "\t\t\t\t\trow[index] = self.place_to_number[str(value)]\n",
    "\n",
    "\t\treturn row\n",
    "\n",
    "\n",
    "\tdef randomize_df(self):\n",
    "\t\t\n",
    "\n",
    "\t\tlists = []\n",
    "\t\tcolumns = []\n",
    "\n",
    "\t\tcolumns.append(\"acting_character\")\n",
    "\t\tlists.append(self.characters)\n",
    "\n",
    "\t\tfor c in self.characters:\n",
    "\t\t\tc= str(c)\n",
    "\t\t\tcolumns.append(c+\"_alive\")\n",
    "\t\t\tlists.append([0,1])\n",
    "\n",
    "\t\t\tcolumns.append(c+\"_underArrest\")\n",
    "\t\t\tlists.append([0,1])\n",
    "\n",
    "\t\t\tcolumns.append(c+\"_angry\")\n",
    "\t\t\tlists.append([0,1])\n",
    "\n",
    "\t\t\tcolumns.append(c+\"_suspect\")\n",
    "\t\t\tlists.append(self.crimes)\n",
    "\n",
    "\t\t\tcolumns.append(c+\"_at\")\n",
    "\t\t\tlists.append(self.places)\n",
    "\t\t\n",
    "\t\t\t\n",
    "\t\tfor p in self.places:\n",
    "\t\t\tp = str(p)\n",
    "\t\t\tcolumns.append(p+\"_searched\")\n",
    "\t\t\tlists.append([0,1])\n",
    "\n",
    "\t\tcharacters2 = self.characters + ['?']\n",
    "\t\tfor i in self.items:\n",
    "\t\t\ti = str(i)\n",
    "\t\t\tcolumns.append(i+\"_has\")\n",
    "\t\t\tlists.append(characters2 )\n",
    "\n",
    "\t\tfor c in self.crimes:\n",
    "\t\t\tfor i in self.items:\n",
    "\t\t\t\tfor p in self.places:\n",
    "\t\t\t\t\tcolumns.append(str(c)+\"_\"+str(i)+\"_\"+ str(p) +\"_clues\")\n",
    "\t\t\t\t\tlists.append([0,0,0,0,0,0,0,0,0,0,0,1])\n",
    "\t\t\n",
    "\n",
    "\t\td = []\n",
    "\t\tfor l in lists:\n",
    "\t\t\td.append(random.choice(l))\n",
    "\n",
    "\t\t#print(columns)\n",
    "\t\td = ['0','1','0','1','2','2','1','0','0','2','0','1','0','1','2','2','1','0','0','2','2','0','0','0','1','2','0','0','0','0','0','0','0','0','0','0','0','0']\n",
    "\t\tself.df = pd.Series(data = d, index=columns )\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef create_file(self,row):\n",
    "\t\tacting_character = self.acting_character\n",
    "\t\twith open(self.file, 'w') as f:\n",
    "\t\t\tf.write(\"\"\"type item;\n",
    "type place;\n",
    "type basketballPlace : place;\n",
    "type arrestPlace : place;\n",
    "type crime; \n",
    "type citizen : character;\n",
    "type police : character;\n",
    "type detective : police;\n",
    "type inspector : police;\n",
    "\n",
    "property alive(character : character) : boolean;\n",
    "property underArrest(character : character) : number;\n",
    "property angry(character : character) : number;\n",
    "property searched(place : place) : number;\n",
    "property suspect(character : character, c : crime) : boolean;\n",
    "property clue(crime : crime, item : item, place : place) : boolean;\n",
    "property at(character : character) : place;\n",
    "property has(item : item) : character;\n",
    "\n",
    "entity Alice : citizen;\n",
    "entity Bob : citizen;\n",
    "entity Charlie : citizen;\n",
    "entity Sherlock : detective;\n",
    "entity HomeB : place;\n",
    "entity BasketballCourt : basketballPlace;\n",
    "entity Downtown : arrestPlace;\n",
    "entity Basketball : item;\n",
    "entity Bat : item;\n",
    "entity Theft : crime;\n",
    "entity Murder : crime;\n",
    "\n",
    "\t\t   \"\"\"\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\tfor char in self.characters:\n",
    "\t\t\t\tif str(row[char+\"_alive\"]) == '1':\n",
    "\t\t\t\t\tf.write(\"alive(\" + char  +\")  ;\\n\")\n",
    "\t\t\t\tf.write(\"underArrest(\" + char  +\") = \" + str(row[char+\"_underArrest\"]) + \" ;\\n\")\n",
    "\t\t\t\tf.write(\"angry(\" + char  +\") = \" + str(row[char+\"_angry\"]) + \" ;\\n\")\n",
    "\t\t\t\tif str(row[char+\"_suspect\"]) != '2':\n",
    "\t\t\t\t\tf.write(\"suspect(\" + char + ', '+ str(row[char+\"_suspect\"]) +\")   ;\\n\")\n",
    "\t\t\t\tf.write(\"at(\" + char  +\") = \" + str(row[char+\"_at\"]) + \" ;\\n\")\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tfor p in self.places:\n",
    "\t\t\t\tf.write(\"searched(\" + p  +\") = \" + str(row[p+\"_searched\"]) + \" ;\\n\")\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tfor i in self.items:\n",
    "\t\t\t\tf.write(\"has(\" + i  +\") = \" + str(row[i+\"_has\"]) + \" ;\\n\")\n",
    "\t\t\t\t\n",
    "\t\t\tclues = [x for x in row.index if 'clues' in x]\n",
    "\t\t\tfor c in clues:\n",
    "\t\t\t\tif str(row[c]) == '1':\n",
    "\t\t\t\t\tentities = c.split('_')\n",
    "\t\t\t\t\tf.write(\"clue(\" + entities[0]+\",\"+entities[1]+\",\"+ entities[2] +\")   ;\\n\")\n",
    "\t\t\t\t#warrtosci postaci\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\tf.write(\"\"\"\n",
    "action travel(character : character, from : place, to : place){\n",
    "\tprecondition:\n",
    "\t\tfrom != to & \n",
    "\t\tat(character) == from &\n",
    "\t\talive(character);\n",
    "\teffect:\n",
    "\t\tat(character) = to;\n",
    "\tconsenting: character; \n",
    "\tobserving(c : character) : at(c) == from | at(c) == to; \n",
    "};\n",
    "\n",
    "action arrest(police : police, character : character, place : place, crime : crime){\n",
    "\tprecondition: \n",
    "\t\tat(police) == place &\n",
    "\t\tat(character) == place &\n",
    "\t\tpolice != character &\n",
    "\t\talive(police) &\n",
    "\t\talive(character) &\n",
    "\t\tsuspect(character, crime);\n",
    "\teffect:\n",
    "\t\tunderArrest(character) = 1;\n",
    "\tconsenting: police;\n",
    "\tobserving(a : character) : at(a) == place;\n",
    "};\n",
    "\n",
    "action steal(thief : citizen, victim : citizen, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(thief) == place &\n",
    "\t\tat(victim) == place &\n",
    "\t\thas(item) == victim &\n",
    "\t\tthief != victim &\n",
    "\t\talive(thief);\n",
    "\teffect:\n",
    "\t\thas(item) = thief &\n",
    "\t\tangry(victim) = 1 &\n",
    "\t\tclue(Theft, item, place);\n",
    "\tconsenting: thief;\n",
    "\tobserving(c : character) : (c == thief | c == victim) | (at(c) == place & place != Downtown); // crimes downtown aren't observed\n",
    "};\n",
    "\n",
    "action play_basketball(player1 : citizen, player2 : citizen, place : basketballPlace){\n",
    "\tprecondition:\n",
    "\t\tplayer1 != player2 &\n",
    "\t\tat(player1) == place &\n",
    "\t\talive(player1) &\n",
    "\t\tat(player2) == place &\n",
    "\t\talive(player2) &\n",
    "\t\thas(Basketball) == player1;\n",
    "\teffect:\n",
    "\t\tangry(player1) = 0 &\n",
    "\t\tangry(player2) = 0;\n",
    "\tconsenting: player1, player2;\n",
    "\tobserving(c : character) : at(c) == place;\n",
    "};\n",
    "\n",
    "action kill(killer : citizen, victim : citizen, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tkiller != victim &\n",
    "\t\tat(killer) == place &\n",
    "\t\tat(victim) == place &\n",
    "\t\talive(killer) &\n",
    "\t\talive(victim) &\n",
    "\t\thas(item) == killer &\n",
    "\t\tunderArrest(killer) == 0;\n",
    "\teffect:\n",
    "\t\t!alive(victim) &\n",
    "\t\tclue(Murder, item, place);\n",
    "\tconsenting: killer;\n",
    "\tobserving(c : character) : c == killer | (at(c) == place & place != Downtown); \n",
    "};\n",
    "\t\n",
    "action find_clues(police : police, crime : crime, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(police) == place &\n",
    "\t\talive(police)\n",
    "\t\t&clue(crime, item, place);\n",
    "\teffect:\n",
    "\t\tsearched(place) = 1 &\n",
    "\t\tif(clue(crime, item, place))\n",
    "\t\t\tbelieves(police, clue(crime, item, place));\n",
    "\tconsenting: police;\n",
    "\tobserving(c : character) : at(c) == place;\n",
    "};\n",
    "\n",
    "action share_clues(police1 : police, police2 : police, crime : crime, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tpolice1 != police2 &\n",
    "\t\tat(police1) == place &\n",
    "\t\talive(police1) &\n",
    "\t\tat(police2) == place &\n",
    "\t\talive(police2) &\n",
    "\t\tclue(crime, item, place);\n",
    "\teffect:\n",
    "\t\tbelieves(police2, clue(crime, item, place));\n",
    "\tconsenting: police1;\n",
    "\tobserving(c : character) : at(c) == place;\n",
    "};\n",
    " \n",
    "action suspect_of_crime(police : police, citizen : citizen, crime : crime, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tpolice != citizen &\n",
    "\t\tat(police) == place &\n",
    "\t\talive(police) &\n",
    "\t\tat(citizen) == place &\n",
    "\t\talive(citizen) &\n",
    "\t\thas(item) == citizen &\n",
    "\t\texists(p : place) clue(crime, item, p);\n",
    "\teffect:\n",
    "\t\tsuspect(citizen, crime);\n",
    "\tconsenting: police;\n",
    "\tobserving(c : character) : at(c) == place;\n",
    "};\n",
    "\n",
    "trigger see_has(character : character, other : character, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(character) == place &\n",
    "\t\tat(other) == place &\n",
    "\t\thas(item) == other &\n",
    "\t\tbelieves(character, has(item) != other);\n",
    "\teffect:\n",
    "\t\tbelieves(character, has(item) = other);\n",
    "};\n",
    "\n",
    "trigger see_hasnt(character : character, other : character, item : item, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(character) == place &\n",
    "\t\tat(other) == place &\n",
    "\t\thas(item) != other & \n",
    "\t\tbelieves(character, has(item) == other);\n",
    "\teffect:\n",
    "\t\tbelieves(character, has(item) = ?);\n",
    "};\n",
    "\n",
    "trigger see_at(character : character, other : character, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(character) == place &\n",
    "\t\tat(other) == place &\n",
    "\t\tbelieves(character, at(other) != place);\n",
    "\teffect:\n",
    "\t\tbelieves(character, at(other) = place);\n",
    "};\n",
    "\n",
    "trigger see_gone(character : character, other : character, place : place){\n",
    "\tprecondition:\n",
    "\t\tat(character) == place &\n",
    "\t\tat(other) != place &\n",
    "\t\tbelieves(character, at(other) == place);\n",
    "\teffect:\n",
    "\t\tbelieves(character, at(other) = ?);\n",
    "};\n",
    "\n",
    "\t\t\t\"\"\")\n",
    "\t\t   #.format(acting_character)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tf.write(\"utility(): \\n \")\n",
    "\t\t\tf.write(self.utilities[acting_character])\n",
    "\n",
    "\t\t\t\n",
    "\t\t\n",
    "\n",
    "\n",
    "\t\t\tfor char in self.characters:\n",
    "\t\t\t\tf.write(\"utility({}): \\n \".format(char))\n",
    "\t\t\t\tf.write(self.utilities[char])\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tf.close()\n",
    "\n",
    "\n",
    "\tdef _get_obs(self):\n",
    "\t\t\n",
    "\t\tdf2 = self.change_cat_to_number(self.df) \n",
    "\t\tr = df2.to_numpy(dtype='float32')\n",
    "\t\t#print(len(r))\n",
    "\t\treturn r\n",
    "\t\n",
    "\tdef _get_info(self):\n",
    "\t\treturn {0:'test'}\n",
    "\n",
    "\tdef reset(self, seed=None, options=None):\n",
    "\t\t# We need the following line to seed self.np_random\n",
    "\t\tsuper().reset(seed=seed)\n",
    "\t\tprint('reset!')\n",
    "\t\t#randomize row\n",
    "\t\tself.randomize_df()\n",
    "\t\tself.df = self.change_number_to_cat(self.df)\n",
    "\t\tself.create_file(self.df)\n",
    "\t\tself.no_solution = 0\n",
    "\t\tself.score = 0\n",
    "\t\tself.acting_character = self.characters[0]\n",
    "\t\tobservation = self._get_obs() #turn row into observation\n",
    "\t\tinfo = self._get_info() #turn row into info\n",
    "\t\tself.utilities = self.utilities1\n",
    "\t\tif self.render_mode == \"human\":\n",
    "\t\t\tself._render_frame()\n",
    "\n",
    "\t\treturn observation, info\n",
    "\t\n",
    "\t\n",
    "\tdef load_action(self,file):\n",
    "\n",
    "\t\t\n",
    "\t\tp = Popen(['java', '-jar', '..\\lib\\sabre.jar', '-p', file,'-el',\"0\",\"-h\",\"h+\",'-c','n',\"-tl\",\"5000\"], stdout=PIPE, stderr=STDOUT)\n",
    "\t\t#p = Popen(['java', '-jar', 'lib\\sabre.jar', '-p', file,'-el',\"0\",'-g',\"\",\"-tl\",\"1000\"], stdout=PIPE, stderr=STDOUT)\n",
    "\n",
    "\t\tlines=[]\n",
    "\t\tfor line in p.stdout:\n",
    "\t\t\tlines.append(str(line, encoding='utf-8'))\n",
    "\n",
    "\t\t#print(lines)\n",
    "\t\treturn lines[0].replace(\"\\r\\n\",\"\")\n",
    "\n",
    "\tdef do_action(self,args):\n",
    "\t\tif len(args) > 0 and len(self.df_effects[self.df_effects.action == args[0] ]['effect_function'].values) > 0:\n",
    "\t\t\tfunctions = self.df_effects[self.df_effects.action == args[0] ]['effect_function'].values[0].split(';')\n",
    "\t\t\tfor function in functions:\n",
    "\t\t\t\tparts = function.split(':')\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t#choosing feature\n",
    "\t\t\t\tfeature = parts[0]\n",
    "\n",
    "\t\t\t\t#choosing how feature is changed\n",
    "\t\t\t\tchange = parts[1]\n",
    "\n",
    "\t\t\t\tfor count,arg in enumerate(args,0):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\tfeature = feature.replace('arg'+str(count),arg)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\tchange = change.replace('arg'+str(count),arg)\n",
    "\n",
    "\t\t\t\tchange = change.split(\"_\")\n",
    "\t\t\t\tprint(feature)\n",
    "\t\t\t\tprint(change)\n",
    "\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tif change[0] == \"=\":\n",
    "\t\t\t\t\tprint(\"!\")\n",
    "\t\t\t\t\tprint(change[1])\n",
    "\t\t\t\t\tself.df[feature] = change[1]\n",
    "\t\t\t\t\tprint(self.df[feature])\n",
    "\t\t\t\telif change[0] == \"+\":\n",
    "\t\t\t\t\tself.df[feature] = int(self.df[feature].values[0]) + int(change[1])\n",
    "\t\t\t\telif change[0] == \"-\":\n",
    "\t\t\t\t\tself.df[feature] = int(self.df[feature]) - int(change[1])\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\t\t#print(self.df.index)\n",
    "\t\t\t\tself.df = self.change_cat_to_number(self.df)\n",
    "\t\t\t\n",
    "\n",
    "\tdef change_state(self,actions):\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tif 'No solution' not in actions:\n",
    "\t\t\tactions = actions.split(') ')\n",
    "\t\t\t\n",
    "\t\t\tif len(actions) > 0:\n",
    "\t\t\t\targs = actions[0].replace(\"(\",\" \").replace(\")\",\"\").replace(\",\",\"\")\n",
    "\t\t\t\tprint(args)\n",
    "\t\t\t\tif ('key_action' not in args) :\n",
    "\t\t\t\t\tself.do_action(args.split(\" \"))\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn True\n",
    "\t\t\n",
    "\t\treturn False\n",
    "\n",
    "\tdef make_action(self,action):\n",
    "\t\tif 'char' in action:\n",
    "\t\t\tchar = action.split('char')[1]\n",
    "\t\t\tself.acting_character = self.number_to_character[str(char)]\n",
    "\t\t\tprint(self.acting_character)\n",
    "\t\telif 'Sherlock_Utility_1' in action:\n",
    "\t\t\tself.utilities = self.utilities1\n",
    "\t\t\t#self.additional_utility = action\n",
    "\t\telif 'Sherlock_Utility_2' in action:\n",
    "\t\t\tself.utilities = self.utilities2\n",
    "\t\telse: print(\"nie ma takiej akcji!\")\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself.create_file(self.change_number_to_cat(self.df))\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself.results = self.load_action(self.file)\n",
    "\t\t\n",
    "\n",
    "\t\tend = time.time()\n",
    "\t\t\n",
    "\t\tindex = 0\n",
    "\t\t#self.df.loc[index,['results']] = self.results\n",
    "\t\t#self.df.loc[index,['time']] = (end-start)\n",
    "\n",
    "\t\tprint(self.results)\n",
    "\t\tself.change_state(self.results)\n",
    "\t\t\n",
    "\tdef get_reward(self):\n",
    "\t\t#stworzenie targetu na poczatku\n",
    "\t\treward = 0\n",
    "\t\t#czy osiagnelismy target jesli tak to 1\n",
    "\t\tsum = int(self.df['Alice_underArrest']) + int(self.df['Bob_underArrest']) + int(self.df['Charlie_underArrest'])\n",
    "\t\tif self.df['Charlie_underArrest'] == '1':\n",
    "\t\t\treward = 1\n",
    "\t\telif 'No solution' in self.results or 'Time limit' in self.results:\n",
    "\t\t\treward = -0.25\n",
    "\t\t\tself.no_solution = self.no_solution+1\n",
    "\t\telse: \n",
    "\t\t\tself.no_solution = 0\n",
    "\t\t\treward = -1\n",
    "\t\t#jesli nie to 0\n",
    "\t\t#jesli nie działa to minus\n",
    "\t\t#jesli to działa, to ustawienie samemu targetu, też w tabeli\n",
    "\t\t\n",
    "\t\treturn reward\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\t# Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "\t\tdirection = self._action_to_direction[action]\n",
    "\t\t\n",
    "\t\tself.make_action(direction)\n",
    "\n",
    "\t\t\n",
    "\t\tterminated = False\n",
    "\t\treward = self.get_reward()\n",
    "\t\tif reward > 0 :\n",
    "\t\t\tterminated = True\n",
    "\t\tif self.no_solution > 6:\n",
    "\t\t\tterminated = True\n",
    "\t\tself.score += reward\n",
    "\t\tobservation = self._get_obs()\n",
    "\t\tinfo = self._get_info()\n",
    "\n",
    "\t\t#if self.render_mode == \"human\":\n",
    "\t\t\t\n",
    "\n",
    "\t\treturn observation, reward, terminated, self.score, info\n",
    "\n",
    "\tdef render(self):\n",
    "\t\tprint(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BasketballEnv at 0x243bcf00a90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasketballEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, epsilon=1e-6, alpha=0.8, beta=0.4, beta_increment=0.001):\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha   # how much prioritisation is used\n",
    "        self.beta = beta    # for importance sampling weights\n",
    "        self.beta_increment = beta_increment\n",
    "        self.priority_buffer = np.zeros(self.capacity)\n",
    "        self.data = []\n",
    "        self.position = 0\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def push(self, experience):\n",
    "        max_priority = np.max(self.priority_buffer) if self.data else 1.0\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(experience)\n",
    "        else:\n",
    "            self.data[self.position] = experience\n",
    "        self.priority_buffer[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        priorities = self.priority_buffer[:len(self.data)]\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.data), batch_size, p=probabilities)\n",
    "        experiences = [self.data[i] for i in indices]\n",
    "\n",
    "        total = len(self.data)\n",
    "        weights = (total * probabilities[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment])\n",
    "        \n",
    "        return experiences, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priority_buffer[idx] = error + self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_shape, action_size, learning_rate_max=0.001, learning_rate_decay=0.995, gamma=0.75, \n",
    "                 memory_size=2000, batch_size=32, exploration_max=1.0, exploration_min=0.01, exploration_decay=0.995):\n",
    "        self.state_shape = state_shape\n",
    "        self.state_tensor_shape = (-1,) + state_shape\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate_max = learning_rate_max\n",
    "        self.learning_rate = learning_rate_max\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.gamma = gamma\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = PrioritizedReplayBuffer(capacity=2000)\n",
    "        self.batch_size = batch_size\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # the actual neural network structure\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape=self.state_shape))\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(tf.keras.layers.Dropout(0.1))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear', name='action_values', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon=None):\n",
    "        if epsilon == None:\n",
    "            epsilon = self.exploration_rate\n",
    "        if np.random.rand() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return np.argmax(self.target_model.predict(state, verbose=0)[0])\n",
    "    \n",
    "    def replay(self, episode=0):\n",
    "\n",
    "        if self.memory.length() < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        experiences, indices, weights = self.memory.sample(self.batch_size)\n",
    "        unpacked_experiences = list(zip(*experiences))\n",
    "        states, actions, rewards, next_states, dones = [list(arr) for arr in unpacked_experiences]\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = tf.convert_to_tensor(states)\n",
    "        states = tf.reshape(states, self.state_tensor_shape)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states)\n",
    "        next_states = tf.reshape(next_states, self.state_tensor_shape)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # Compute Q values and next Q values\n",
    "        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "\n",
    "        # Compute target values using the Bellman equation\n",
    "        max_target_q_values = np.max(target_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_target_q_values\n",
    "\n",
    "        # Compute TD errors\n",
    "        batch_indices = np.arange(self.batch_size)\n",
    "        q_values_current_action = q_values[batch_indices, actions]\n",
    "        td_errors = targets - q_values_current_action\n",
    "        self.memory.update_priorities(indices, np.abs(td_errors))\n",
    "\n",
    "        # For learning: Adjust Q values of taken actions to match the computed targets\n",
    "        q_values[batch_indices, actions] = targets\n",
    "\n",
    "        loss = self.model.train_on_batch(states, q_values, sample_weight=weights)\n",
    "\n",
    "        self.exploration_rate = self.exploration_max*self.exploration_decay**episode\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        self.learning_rate = self.learning_rate_max*self.learning_rate_decay**episode\n",
    "        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.learning_rate)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model = tf.keras.models.load_model(name)\n",
    "        self.target_model = tf.keras.models.load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset!\n",
      "reset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kacpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\shape_base.py:591: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  a = asanyarray(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "Time limit reached.\n",
      "Time limit reached.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'next_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m     41\u001b[0m observation, reward, done, score, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 42\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[43mnext_state\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     43\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[0;32m     44\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mNameError\u001b[0m: name 'next_state' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "EXPLORATION_DECAY = 0.95\n",
    "GAMMA = 0.975\n",
    "UPDATE_TARGET_EVERY = 10\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPISODES = 101\n",
    "\n",
    "env = BasketballEnv() \n",
    "state, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "agent = DQN(\n",
    "    state_shape=state.shape,\n",
    "    action_size=env.action_space.n,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate_max=LEARNING_RATE,\n",
    "    learning_rate_decay=LEARNING_RATE_DECAY,\n",
    "    exploration_decay=EXPLORATION_DECAY,\n",
    "    gamma=GAMMA\n",
    ")\n",
    "agent.save(f'models/-1.h5')\n",
    "\n",
    "state = env.reset()\n",
    "state = np.expand_dims(state, axis=0)\n",
    "\n",
    "most_recent_losses = deque(maxlen=BATCH_SIZE)\n",
    "\n",
    "log = []\n",
    "\n",
    "# fill up memory before training starts\n",
    "while agent.memory.length() < BATCH_SIZE:\n",
    "    action = agent.act(state)\n",
    "    observation, reward, done, score, info = env.step(action)\n",
    "    next_state = np.expand_dims(next_state, axis=0)\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "for e in range(0, EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    done = False\n",
    "    step = 0\n",
    "    ma_loss = None\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        observation, reward, done, score, info = env.step(action)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "        loss = agent.replay(episode=e)\n",
    "        most_recent_losses.append(loss)\n",
    "        ma_loss = np.array(most_recent_losses).mean()\n",
    "\n",
    "        if loss != None:\n",
    "            print(f\"Step: {step}. Score: {score}. -- Loss: {loss}\", end=\"          \\r\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {e}/{EPISODES-1} completed with {step} steps. Score: {score:.0f}. LR: {agent.learning_rate:.6f}. EP: {agent.exploration_rate:.2f}. MA loss: {ma_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    log.append([e, step, score, agent.learning_rate, agent.exploration_rate, ma_loss])\n",
    "\n",
    "    agent.save(f'models/{e}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
